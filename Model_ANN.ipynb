{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport datetime\nimport gc\nimport os \n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import mean_squared_error\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n%matplotlib inline\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso,LogisticRegression)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        \n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef missing_statistics(df):    \n    statitics = pd.DataFrame(df.isnull().sum()).reset_index()\n    statitics.columns=['COLUMN NAME',\"MISSING VALUES\"]\n    statitics['TOTAL ROWS'] = df.shape[0]\n    statitics['% MISSING'] = round((statitics['MISSING VALUES']/statitics['TOTAL ROWS'])*100,2)\n    return statitics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = import_data('/kaggle/input/testval-processing/df_train.csv')\ndf_val = import_data('/kaggle/input/testval-processing/df_val.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = import_data('/kaggle/input/testval-processing/data_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#training['timestamp'] = pd.to_datetime(training.timestamp, format='%Y-%m-%d %H:%M:%S')\n#training['timestamp'] = pd.to_numeric(training['timestamp'])\ntraining.primary_use = le.fit_transform(training.primary_use)\ntraining.set_index(['hour', 'month'], inplace=True)\n\n#data_test['timestamp'] = pd.to_datetime(data_test.timestamp, format='%Y-%m-%d %H:%M:%S')\n#data_test['timestamp'] = pd.to_numeric(data_test['timestamp'])\ndata_test.primary_use = le.fit_transform(data_test.primary_use)\ndata_test.set_index(['hour', 'month'], inplace=True)\ntraining","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\n#training = training.sample(frac = 1) \n\nx_train = training[['site_id','building_id','meter','primary_use','square_feet','air_temperature',\n'dew_temperature','wind_direction','wind_speed','sea_level_pressure','precip_depth_1_hr']]\n#\ny_train = training['meter_reading'] \n\nX_val = training[['site_id','building_id','meter','primary_use','square_feet','air_temperature',\n'dew_temperature','wind_direction','wind_speed','sea_level_pressure','precip_depth_1_hr']]\n#\ny_val = training['meter_reading'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.values[:]\nx_train= x_train.reshape((x_train.shape[0],1,x_train.shape[-1]))\n#x_train = np.expand_dims(X,axis = 2)\ny_train = y_train.values\n\n# validation_data=(x_val,y_val)\nx_val = X_val.values[:]\nx_val = x_val.reshape((x_val.shape[0],1,x_val.shape[-1]))\n#x_val = np.expand_dims(X,axis = 2)\ny_val= y_val.values\n# print(x_train.shape , x_val.shape) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ANN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Dense, LSTM, GRU, Dropout, BatchNormalization \nfrom keras.layers.core import Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop,Adam\nfrom keras import regularizers\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers \nimport random\nimport os\n\n# Import deep learning layers and functions from tensorflow\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set all random values of the system\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(1)\n   print('tf')\n   tf.random.set_seed(1)\n   print('tf')\n   np.random.seed(1)\n   print('np')\n   random.seed(1)\n   print('np')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train, x_val, y_train, y_val = train_test_split(X,y, test_size = 0.2, random_state= 45 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path=\"Best_model/cp-{epoch:04d}.ckpt\" \n#checkpoint_path=\"Best_model/cp.ckpt\" # The number of the best epoch is also specified in the ast model path\n#-{epoch:04d}\ncheckpoint_dir = os.path.dirname(checkpoint_path)\ncheckpoint = tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, \n                                                        save_best_only=True, mode='min')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_root_mean_squared_error', min_delta=0.0001, patience=1, verbose=True, mode='auto')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=0.01) # model optimazer with mearning rat eof 0,01\n\ncallbacks_list =[tf.keras.callbacks.EarlyStopping(patience=4,verbose=True, mode='auto'),checkpoint]  # put the checkpoint belong the callbacks_list\n\n# tf.keras.callbacks.TensorBoard(log_dir='./logs') #Write TensorBoard logs after every batch of training to monitor your metrics\n\nmy_init = tensorflow.keras.initializers.glorot_uniform(seed=1) # features weights initializer\n# Use tensorflow.keras.layers\nmodel = tensorflow.keras.Sequential([\n            tensorflow.keras.layers.Dense(200, input_dim=x_train.shape[-1], activation='relu',kernel_initializer=my_init),\n            tensorflow.keras.layers.Dense(128, activation='relu',kernel_initializer=my_init),\n            tensorflow.keras.layers.Dense(64, activation='relu',kernel_initializer=my_init),\n            tensorflow.keras.layers.Dense(1,kernel_initializer=my_init)\n        ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=opt, loss='mean_absolute_error') #Adam(learning_rate=0.001) : this is the default learning rate of adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train, epochs=100, batch_size=6000, validation_data=(x_val,y_val) ,verbose=1,callbacks=callbacks_list); # include callbacks_list to callbacks parameter\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del training , checkpoint , x_train , y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls {checkpoint_dir}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = os.listdir('Best_model')\nmodels.sort()\nm = 'Best_model/'+models[-1]\nBest_model = tensorflow.keras.models.load_model(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import sqrt\nmse = Best_model.evaluate(x_val, y_val, verbose=0)\nprint('MSE: %.3f, RMSE: %.3f ' % (mse, sqrt(mse)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x_val , y_val , models , m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = data_test[['site_id','building_id','meter','primary_use','square_feet','air_temperature',\n                 'dew_temperature','sea_level_pressure','wind_direction','wind_speed','precip_depth_1_hr']]\n\ntest_data = test_data.values[:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" submission=import_data('/kaggle/input/ashrae-energy-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_predict=Best_model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(f_predict[f_predict>1])\nnp.count_nonzero(~np.isnan(f_predict)) #number of value non Nan\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.concatenate((test_data, f_predict), axis=1)\ndel f_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_pfs = pd.DataFrame(test,columns=['site_id','building_id','meter','primary_use','square_feet','air_temperature',\n                 'dew_temperature','sea_level_pressure','wind_direction','wind_speed','precip_depth_1_hr','meter_reading'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_to_convtest = submission_pfs[submission_pfs['site_id']==0].index #Convertir les pr√©dictions du site_id  0 \n\nfor j in data_to_convtest :\n    submission_pfs.at[j,'meter_reading']=(submission_pfs.at[j,'meter_reading']*3.4118)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submition data"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['meter_reading'] = submission_pfs['meter_reading'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data_to_convtest, submission_pfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['meter_reading'].clip(lower=0,upper=None,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('ANN_sub.csv',index = False)\nsubmission.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}